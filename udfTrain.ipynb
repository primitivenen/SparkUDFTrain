{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/temporary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib # To Pickel Trained model file \n",
    "import numpy as np # To create random data\n",
    "import pandas as pd # To operate on data in Python Process\n",
    "from sklearn.linear_model import LinearRegression # To train Linear Regression models\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType # Pandas UDF functions to call Python processes from spark\n",
    "from pyspark.sql.types import DoubleType, StringType, ArrayType # Data types to capture reurn at Spark End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'x': np.random.normal(size=100)})\n",
    "df1['y'] = df1['x']*2.5 + np.random.normal(scale=0.5, size=100) # DF1 is dummy Linear data Y = 2.5*x + random noise of 100 datapoints\n",
    "df1['name'] = 'df1'\n",
    "df1.to_csv('df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "gzip -f \"df1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mv(\"file:/databricks/driver/df1.csv.gz\", \"dbfs:/FileStore/temporary/df1.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'x': np.random.normal(size=100)})\n",
    "df2['y'] = df2['x']*3.0 + np.random.normal(scale=0.3, size=100) # DF2 is dummy Linear data Y = 3.0*x + random noise of 100 datapoints\n",
    "df2['name'] = 'df2'\n",
    "df2.to_csv('df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "gzip -f \"df2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.fs.mv(\"file:/databricks/driver/df2.csv.gz\", \"dbfs:/FileStore/temporary/df2.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls dbfs:/FileStore/temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF = (spark.read\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"delimiter\", \",\")\n",
    "            .option(\"inferSchema\", \"true\") \n",
    "            .csv('dbfs:/FileStore/temporary/df*.csv.gz'))\n",
    "\n",
    "sparkDF.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(returnType=DoubleType())\n",
    "def train_lm_pandas_udf(*cols):\n",
    "    df = pd.concat(cols, axis=1) # Create pandas dataframe using input Spark DataFrame columns\n",
    "    df.columns = ['x', 'y', 'name']\n",
    "    modelUDF = LinearRegression() # Scikit-Learn Linear Regression \n",
    "    modelUDF.fit(pd.DataFrame(df['x']),df['y']) # Fit Scikit-Learn Linear Regression Model\n",
    "    sig = df.loc[0,'name'] # Unique Identiter for model files, obtained from one of the columns in dataset\n",
    "    joblib.dump(modelUDF, 'modelUDF{signature}.joblib'.format(signature=sig)) # Pickel Thetrained model file\n",
    "    return pd.Series(modelUDF.predict(pd.DataFrame(df['x']))) # Returns Predicted values on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['x', 'y', 'name']\n",
    "sparkDF2 = sparkDF.select(train_lm_pandas_udf(*column_names).alias(\"TrainPrediction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF2.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%fs ls file:/databricks/driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldf1 = joblib.load('modelUDFdf1.joblib')\n",
    "modeldf2 = joblib.load('modelUDFdf2.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldf1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldf2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "udfTrain",
  "notebookId": 4386531222885202
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
